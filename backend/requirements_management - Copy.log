2025-10-19 17:20:56 - requirements_management - [32mINFO[0m - build_graph:39 - Building workflow graph
2025-10-19 17:20:56 - requirements_management - [32mINFO[0m - build_graph:73 - Workflow graph built successfully
2025-10-19 17:21:09 - requirements_management - [32mINFO[0m - build_graph:39 - Building workflow graph
2025-10-19 17:21:09 - requirements_management - [32mINFO[0m - build_graph:73 - Workflow graph built successfully
2025-10-19 17:21:19 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:21:19 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:21:28 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:21:28 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:21:28 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:24:36 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:24:36 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:24:36 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:26:24 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:26:24 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:26:24 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:30:43 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:30:43 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:30:43 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:31:12 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:31:12 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:33:55 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:33:55 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:33:55 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:34:04 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:34:04 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:34:04 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:34:12 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:34:12 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:34:12 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:34:39 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:34:39 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:34:39 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:36:17 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_3a4e0f81
2025-10-19 17:36:17 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:36:18 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:36:29 - requirements_management - [32mINFO[0m - select_keyword:106 - Selected keyword: ergonomic infotainment interior
2025-10-19 17:36:29 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:36:29 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:36:29 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:36:30 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:38:00 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:38:00 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:38:00 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:38:10 - requirements_management - [32mINFO[0m - build_graph:28 - Building workflow graph
2025-10-19 17:38:10 - requirements_management - [32mINFO[0m - build_graph:55 - Graph built successfully
2025-10-19 17:38:10 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:38:18 - requirements_management - [32mINFO[0m - build_graph:35 - Building workflow graph
2025-10-19 17:38:18 - requirements_management - [32mINFO[0m - build_graph:69 - Graph built successfully
2025-10-19 17:38:18 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:38:31 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_2bf25f6c
2025-10-19 17:38:31 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:38:33 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:38:37 - requirements_management - [32mINFO[0m - select_keyword:106 - Selected keyword: fuel efficient engines
2025-10-19 17:38:37 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:38:39 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:38:39 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:38:40 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:39:07 - requirements_management - [32mINFO[0m - build_graph:35 - Building workflow graph
2025-10-19 17:39:07 - requirements_management - [32mINFO[0m - build_graph:69 - Graph built successfully
2025-10-19 17:39:07 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:39:21 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_eb3b0724
2025-10-19 17:39:21 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:39:23 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:39:26 - requirements_management - [32mINFO[0m - select_keyword:106 - Selected keyword: scalable manufacturing processes
2025-10-19 17:39:26 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:39:27 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["high safety standards","fuel efficient engines","comfortable ergonomic interior","scalable manufacturing processes","robust after sales"]'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["high safety standards","fuel efficient engines","comfortable ergonomic interior","scalable manufacturing processes","robust after sales"]'}}
2025-10-19 17:39:27 - requirements_management - [31mERROR[0m - select_keyword:135 - Error selecting keyword: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["high safety standards","fuel efficient engines","comfortable ergonomic interior","scalable manufacturing processes","robust after sales"]'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 111, in select_keyword
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["high safety standards","fuel efficient engines","comfortable ergonomic interior","scalable manufacturing processes","robust after sales"]'}}
During task with name 'generate_keywords' and id '52bdba47-feee-195a-05ba-bf3c6e788692'
2025-10-19 17:40:02 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_a5987521
2025-10-19 17:40:02 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:40:02 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:40:06 - requirements_management - [32mINFO[0m - select_keyword:106 - Selected keyword: robust after-sales services
2025-10-19 17:40:06 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:40:07 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 17:40:07 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 17:40:08 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after-sales service"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after-sales service"\n]\n```'}}
2025-10-19 17:40:08 - requirements_management - [31mERROR[0m - select_keyword:135 - Error selecting keyword: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after-sales service"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 117, in select_keyword
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after-sales service"\n]\n```'}}
During task with name 'generate_keywords' and id '0d8aa485-f26d-c0b1-2e23-b1d7e3b692eb'
2025-10-19 17:40:45 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:40:45 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:40:45 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:41:08 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:41:08 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:41:08 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:41:40 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:41:40 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:41:40 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:42:07 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_6375b7af
2025-10-19 17:42:08 - requirements_management - [32mINFO[0m - generate_keywords:29 - Generating keywords
2025-10-19 17:42:08 - requirements_management - [32mINFO[0m - generate_keywords:60 - Generated 5 keywords
2025-10-19 17:42:12 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: energy saving technology
2025-10-19 17:42:12 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 17:42:12 - requirements_management - [32mINFO[0m - generate_requirements:72 - Generating requirements for: energy saving technology
2025-10-19 17:42:13 - requirements_management - [32mINFO[0m - generate_requirements:109 - Generated 5 requirements
2025-10-19 17:42:13 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 17:42:13 - requirements_management - [32mINFO[0m - generate_risks:124 - Generating risks
2025-10-19 17:42:15 - requirements_management - [32mINFO[0m - generate_risks:166 - Generated 5 risks
2025-10-19 17:42:25 - requirements_management - [32mINFO[0m - save_project:204 - Saving project project_6375b7af
2025-10-19 17:42:25 - requirements_management - [32mINFO[0m - call_save_tool:181 - Saving to Neo4j
2025-10-19 17:42:25 - requirements_management - [32mINFO[0m - save_to_neo4j:26 - Saving to Neo4j: project_6375b7af
2025-10-19 17:42:26 - requirements_management - [32mINFO[0m - save_to_neo4j:64 - Saved 5 requirements, 5 risks
2025-10-19 17:42:26 - requirements_management - [32mINFO[0m - call_save_tool:192 - Saved successfully
2025-10-19 17:47:17 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:47:17 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:47:17 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:48:07 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_b34885e6
2025-10-19 17:48:07 - requirements_management - [32mINFO[0m - generate_keywords:29 - Generating keywords
2025-10-19 17:48:08 - requirements_management - [31mERROR[0m - generate_keywords:67 - Error generating keywords: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "adaptive display brightness",\n    "contrast ratio compliance",\n    "extreme lighting readability",\n    "fuel efficiency alerts",\n    "usability testing scenarios"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "adaptive display brightness",\n    "contrast ratio compliance",\n    "extreme lighting readability",\n    "fuel efficiency alerts",\n    "usability testing scenarios"\n  ]\n}}'}}
2025-10-19 17:48:08 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "adaptive display brightness",\n    "contrast ratio compliance",\n    "extreme lighting readability",\n    "fuel efficiency alerts",\n    "usability testing scenarios"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "adaptive display brightness",\n    "contrast ratio compliance",\n    "extreme lighting readability",\n    "fuel efficiency alerts",\n    "usability testing scenarios"\n  ]\n}}'}}
During task with name 'generate_keywords' and id '6c2c6a03-d66d-a46b-16e4-18e077f37c61'
2025-10-19 17:48:43 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_e88b8879
2025-10-19 17:48:43 - requirements_management - [32mINFO[0m - generate_keywords:29 - Generating keywords
2025-10-19 17:48:44 - requirements_management - [31mERROR[0m - generate_keywords:67 - Error generating keywords: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
2025-10-19 17:48:44 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
During task with name 'generate_keywords' and id '5e202540-6b8f-bf29-6219-a767e2301c10'
2025-10-19 17:48:55 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_b052f1f8
2025-10-19 17:48:55 - requirements_management - [32mINFO[0m - generate_keywords:29 - Generating keywords
2025-10-19 17:48:58 - requirements_management - [31mERROR[0m - generate_keywords:67 - Error generating keywords: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "optimal performance interior",\n    "scalable manufacturing quality",\n    "after sales support"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "optimal performance interior",\n    "scalable manufacturing quality",\n    "after sales support"\n  ]\n}}'}}
2025-10-19 17:48:58 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "optimal performance interior",\n    "scalable manufacturing quality",\n    "after sales support"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'JSON' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "JSON", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "optimal performance interior",\n    "scalable manufacturing quality",\n    "after sales support"\n  ]\n}}'}}
During task with name 'generate_keywords' and id '4f800a13-f10c-0395-53b5-640317724df8'
2025-10-19 17:49:15 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:49:15 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:49:15 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:49:20 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_194ce302
2025-10-19 17:49:21 - requirements_management - [32mINFO[0m - generate_keywords:29 - Generating keywords
2025-10-19 17:49:29 - requirements_management - [31mERROR[0m - generate_keywords:67 - Error generating keywords: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
2025-10-19 17:49:29 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust after-sales service"\n  ]\n}}'}}
During task with name 'generate_keywords' and id 'e08b1f38-a85a-e3fa-86b2-b5163bcb6d2e'
2025-10-19 17:51:21 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:51:21 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:51:21 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:51:29 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_1f0d6802
2025-10-19 17:51:29 - requirements_management - [32mINFO[0m - generate_keywords:29 - Generating keywords
2025-10-19 17:51:30 - requirements_management - [31mERROR[0m - generate_keywords:67 - Error generating keywords: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust aftersales service"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust aftersales service"\n  ]\n}}'}}
2025-10-19 17:51:30 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust aftersales service"\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 54, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "keywords": [\n    "fuel efficient engines",\n    "low emission compliance",\n    "ergonomic interior comfort",\n    "scalable manufacturing processes",\n    "robust aftersales service"\n  ]\n}}'}}
During task with name 'generate_keywords' and id 'f6d5a39b-875b-43b5-b626-0c79f7731737'
2025-10-19 17:55:34 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:55:34 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:55:34 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:55:43 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_4b313aa7
2025-10-19 17:55:43 - requirements_management - [32mINFO[0m - generate_keywords:32 - Generating keywords
2025-10-19 17:55:43 - requirements_management - [31mERROR[0m - generate_keywords:70 - Error generating keywords: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 57, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-19 17:55:43 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 57, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
During task with name 'generate_keywords' and id 'ebe2e66f-8ba9-78f0-a017-ac0b150ea96c'
2025-10-19 17:56:56 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:56:56 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:56:56 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:57:04 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_a144b82f
2025-10-19 17:57:04 - requirements_management - [32mINFO[0m - generate_keywords:32 - Generating keywords
2025-10-19 17:57:04 - requirements_management - [31mERROR[0m - generate_keywords:70 - Error generating keywords: Error code: 404 - {'error': {'message': 'The model `llama-3-groq-70b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 57, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `llama-3-groq-70b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}
2025-10-19 17:57:05 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 404 - {'error': {'message': 'The model `llama-3-groq-70b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 57, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `llama-3-groq-70b-tool-use` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}
During task with name 'generate_keywords' and id 'e5f8f614-753a-5cd8-9ab8-16bebec9985e'
2025-10-19 17:58:35 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:58:35 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:58:35 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:58:49 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_931f5b4f
2025-10-19 17:58:49 - requirements_management - [32mINFO[0m - generate_keywords:32 - Generating keywords
2025-10-19 17:58:49 - requirements_management - [31mERROR[0m - generate_keywords:70 - Error generating keywords: Error code: 400 - {'error': {'message': 'json mode cannot be combined with tool/function calling', 'type': 'invalid_request_error'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 57, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'json mode cannot be combined with tool/function calling', 'type': 'invalid_request_error'}}
2025-10-19 17:58:49 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': 'json mode cannot be combined with tool/function calling', 'type': 'invalid_request_error'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 57, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'json mode cannot be combined with tool/function calling', 'type': 'invalid_request_error'}}
During task with name 'generate_keywords' and id 'aa33bc33-3d58-f69f-2214-cab623960dd6'
2025-10-19 17:59:15 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 17:59:15 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 17:59:15 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 17:59:18 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_f7e2d11b
2025-10-19 17:59:19 - requirements_management - [32mINFO[0m - generate_keywords:31 - Generating keywords
2025-10-19 17:59:20 - requirements_management - [31mERROR[0m - generate_keywords:69 - Error generating keywords: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic infotainment systems",\n    "scalable quality manufacturing",\n    "climate control integration",\n    "warranty customer support"\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 56, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic infotainment systems",\n    "scalable quality manufacturing",\n    "climate control integration",\n    "warranty customer support"\n  ]\n}'}}
2025-10-19 17:59:20 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic infotainment systems",\n    "scalable quality manufacturing",\n    "climate control integration",\n    "warranty customer support"\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 56, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic infotainment systems",\n    "scalable quality manufacturing",\n    "climate control integration",\n    "warranty customer support"\n  ]\n}'}}
During task with name 'generate_keywords' and id '0e84d215-d160-bfdc-df00-cb8ce520d303'
2025-10-19 18:01:30 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:01:30 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:01:30 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:01:46 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_733cf82e
2025-10-19 18:01:46 - requirements_management - [32mINFO[0m - generate_keywords:31 - Generating keywords
2025-10-19 18:01:47 - requirements_management - [31mERROR[0m - generate_keywords:69 - Error generating keywords: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic interior comfort",\n    "scalable production quality",\n    "modern connectivity systems",\n    "reliable after-sales support"\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 56, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic interior comfort",\n    "scalable production quality",\n    "modern connectivity systems",\n    "reliable after-sales support"\n  ]\n}'}}
2025-10-19 18:01:48 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic interior comfort",\n    "scalable production quality",\n    "modern connectivity systems",\n    "reliable after-sales support"\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 56, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "ergonomic interior comfort",\n    "scalable production quality",\n    "modern connectivity systems",\n    "reliable after-sales support"\n  ]\n}'}}
During task with name 'generate_keywords' and id '47d3544b-2d3f-4d75-6423-a03e3a851ad0'
2025-10-19 18:02:43 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:02:43 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:02:43 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:03:07 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_15f10bb2
2025-10-19 18:03:07 - requirements_management - [32mINFO[0m - generate_keywords:31 - Generating keywords
2025-10-19 18:03:08 - requirements_management - [31mERROR[0m - generate_keywords:69 - Error generating keywords: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "smart interior comfort",\n    "efficient scalable manufacturing",\n    "reliable after-sales support",\n    "performance emissions balance"\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 56, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "smart interior comfort",\n    "efficient scalable manufacturing",\n    "reliable after-sales support",\n    "performance emissions balance"\n  ]\n}'}}
2025-10-19 18:03:08 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "smart interior comfort",\n    "efficient scalable manufacturing",\n    "reliable after-sales support",\n    "performance emissions balance"\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 56, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "keywords": [\n    "eco-friendly engine tech",\n    "smart interior comfort",\n    "efficient scalable manufacturing",\n    "reliable after-sales support",\n    "performance emissions balance"\n  ]\n}'}}
During task with name 'generate_keywords' and id 'f10dc611-beda-169c-ff0a-700f8db94646'
2025-10-19 18:03:53 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:03:53 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:03:53 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:04:27 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:04:27 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:04:27 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:04:45 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_da32c4b2
2025-10-19 18:04:45 - requirements_management - [32mINFO[0m - generate_keywords:31 - Generating keywords
2025-10-19 18:04:46 - requirements_management - [32mINFO[0m - generate_keywords:50 - Generated 5 keywords
2025-10-19 18:04:52 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable quality manufacturing
2025-10-19 18:04:52 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:04:52 - requirements_management - [32mINFO[0m - generate_requirements:62 - Generating requirements for: scalable quality manufacturing
2025-10-19 18:04:54 - requirements_management - [31mERROR[0m - generate_requirements:109 - Error generating requirements: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "requirements": [\n    "The manufacturing process must support scalable production of at least 50,000 units per quarter while maintaining a defect rate below 0.1%, verified through automated quality inspection systems and monthly audit reports.",\n    "Production facilities must implement ISO 9001-certified quality management systems to ensure consistent adherence to global environmental and safety standards across all scalable manufacturing operations.",\n    "The production line must achieve a 98% throughput efficiency rate when scaling from 10,000 to 100,000 units annually, measured by cycle time metrics and resource utilization analytics.",\n    "All manufacturing data and intellectual property related to engine designs must be protected with AES-256 encryption and role-based access controls to prevent unauthorized access during scalable production phases.",\n    "Operators of scalable manufacturing systems must be trained on intuitive HMI (Human-Machine Interface) platforms that reduce setup time by 30% for new production batches, validated by 95% user satisfaction scores in quarterly usability assessments."\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 92, in generate_requirements
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "requirements": [\n    "The manufacturing process must support scalable production of at least 50,000 units per quarter while maintaining a defect rate below 0.1%, verified through automated quality inspection systems and monthly audit reports.",\n    "Production facilities must implement ISO 9001-certified quality management systems to ensure consistent adherence to global environmental and safety standards across all scalable manufacturing operations.",\n    "The production line must achieve a 98% throughput efficiency rate when scaling from 10,000 to 100,000 units annually, measured by cycle time metrics and resource utilization analytics.",\n    "All manufacturing data and intellectual property related to engine designs must be protected with AES-256 encryption and role-based access controls to prevent unauthorized access during scalable production phases.",\n    "Operators of scalable manufacturing systems must be trained on intuitive HMI (Human-Machine Interface) platforms that reduce setup time by 30% for new production batches, validated by 95% user satisfaction scores in quarterly usability assessments."\n  ]\n}'}}
2025-10-19 18:04:54 - requirements_management - [31mERROR[0m - select_keyword:145 - Error selecting keyword: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "requirements": [\n    "The manufacturing process must support scalable production of at least 50,000 units per quarter while maintaining a defect rate below 0.1%, verified through automated quality inspection systems and monthly audit reports.",\n    "Production facilities must implement ISO 9001-certified quality management systems to ensure consistent adherence to global environmental and safety standards across all scalable manufacturing operations.",\n    "The production line must achieve a 98% throughput efficiency rate when scaling from 10,000 to 100,000 units annually, measured by cycle time metrics and resource utilization analytics.",\n    "All manufacturing data and intellectual property related to engine designs must be protected with AES-256 encryption and role-based access controls to prevent unauthorized access during scalable production phases.",\n    "Operators of scalable manufacturing systems must be trained on intuitive HMI (Human-Machine Interface) platforms that reduce setup time by 30% for new production batches, validated by 95% user satisfaction scores in quarterly usability assessments."\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 125, in select_keyword
    state = generate_requirements(state)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 92, in generate_requirements
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "requirements": [\n    "The manufacturing process must support scalable production of at least 50,000 units per quarter while maintaining a defect rate below 0.1%, verified through automated quality inspection systems and monthly audit reports.",\n    "Production facilities must implement ISO 9001-certified quality management systems to ensure consistent adherence to global environmental and safety standards across all scalable manufacturing operations.",\n    "The production line must achieve a 98% throughput efficiency rate when scaling from 10,000 to 100,000 units annually, measured by cycle time metrics and resource utilization analytics.",\n    "All manufacturing data and intellectual property related to engine designs must be protected with AES-256 encryption and role-based access controls to prevent unauthorized access during scalable production phases.",\n    "Operators of scalable manufacturing systems must be trained on intuitive HMI (Human-Machine Interface) platforms that reduce setup time by 30% for new production batches, validated by 95% user satisfaction scores in quarterly usability assessments."\n  ]\n}'}}
2025-10-19 18:05:47 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:05:47 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:05:47 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:05:55 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_13bd89fd
2025-10-19 18:05:55 - requirements_management - [32mINFO[0m - generate_keywords:31 - Generating keywords
2025-10-19 18:06:00 - requirements_management - [32mINFO[0m - generate_keywords:50 - Generated 5 keywords
2025-10-19 18:06:05 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: reliable after-sales support
2025-10-19 18:06:05 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:06:05 - requirements_management - [32mINFO[0m - generate_requirements:62 - Generating requirements for: reliable after-sales support
2025-10-19 18:06:06 - requirements_management - [32mINFO[0m - generate_requirements:80 - Generated 5 requirements
2025-10-19 18:06:06 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 18:06:06 - requirements_management - [32mINFO[0m - generate_risks:95 - Generating risks
2025-10-19 18:06:09 - requirements_management - [31mERROR[0m - generate_risks:147 - Error generating risks: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "Risks": [\n    "Failure to implement expedited warranty claim processing could lead to customer dissatisfaction and legal disputes due to prolonged resolution times, risking brand reputation and financial penalties.",\n    "Inadequate coverage of certified service centers in remote regions may prevent 24/7 accessibility, resulting in delayed repairs and potential loss of market share in underserved areas.",\n    "AI chatbot inaccuracies or multilingual service gaps in customer support could cause misdiagnosed technical issues and unresolved emergency requests, degrading user trust and support efficiency.",\n    "Predictive inventory analytics might fail to account for sudden regional demand surges, causing critical component stockouts and disrupting repair timelines for vehicle owners.",\n    "Delayed analysis of post-service satisfaction surveys could prevent timely identification of systemic service quality issues, leading to repeated customer complaints and missed SLA resolution targets."\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 131, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "Risks": [\n    "Failure to implement expedited warranty claim processing could lead to customer dissatisfaction and legal disputes due to prolonged resolution times, risking brand reputation and financial penalties.",\n    "Inadequate coverage of certified service centers in remote regions may prevent 24/7 accessibility, resulting in delayed repairs and potential loss of market share in underserved areas.",\n    "AI chatbot inaccuracies or multilingual service gaps in customer support could cause misdiagnosed technical issues and unresolved emergency requests, degrading user trust and support efficiency.",\n    "Predictive inventory analytics might fail to account for sudden regional demand surges, causing critical component stockouts and disrupting repair timelines for vehicle owners.",\n    "Delayed analysis of post-service satisfaction surveys could prevent timely identification of systemic service quality issues, leading to repeated customer complaints and missed SLA resolution targets."\n  ]\n}'}}
2025-10-19 18:06:09 - requirements_management - [31mERROR[0m - select_keyword:145 - Error selecting keyword: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "Risks": [\n    "Failure to implement expedited warranty claim processing could lead to customer dissatisfaction and legal disputes due to prolonged resolution times, risking brand reputation and financial penalties.",\n    "Inadequate coverage of certified service centers in remote regions may prevent 24/7 accessibility, resulting in delayed repairs and potential loss of market share in underserved areas.",\n    "AI chatbot inaccuracies or multilingual service gaps in customer support could cause misdiagnosed technical issues and unresolved emergency requests, degrading user trust and support efficiency.",\n    "Predictive inventory analytics might fail to account for sudden regional demand surges, causing critical component stockouts and disrupting repair timelines for vehicle owners.",\n    "Delayed analysis of post-service satisfaction surveys could prevent timely identification of systemic service quality issues, leading to repeated customer complaints and missed SLA resolution targets."\n  ]\n}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 129, in select_keyword
    state = generate_risks(state)
            ^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 131, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{  "Risks": [\n    "Failure to implement expedited warranty claim processing could lead to customer dissatisfaction and legal disputes due to prolonged resolution times, risking brand reputation and financial penalties.",\n    "Inadequate coverage of certified service centers in remote regions may prevent 24/7 accessibility, resulting in delayed repairs and potential loss of market share in underserved areas.",\n    "AI chatbot inaccuracies or multilingual service gaps in customer support could cause misdiagnosed technical issues and unresolved emergency requests, degrading user trust and support efficiency.",\n    "Predictive inventory analytics might fail to account for sudden regional demand surges, causing critical component stockouts and disrupting repair timelines for vehicle owners.",\n    "Delayed analysis of post-service satisfaction surveys could prevent timely identification of systemic service quality issues, leading to repeated customer complaints and missed SLA resolution targets."\n  ]\n}'}}
2025-10-19 18:07:06 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:07:06 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:07:06 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:07:14 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:07:14 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:07:14 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:07:27 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_a69cd594
2025-10-19 18:07:28 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 18:07:29 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 18:07:32 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing process
2025-10-19 18:07:32 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:07:32 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing process
2025-10-19 18:07:33 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 18:07:33 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 18:07:33 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 18:07:34 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 18:07:46 - requirements_management - [32mINFO[0m - save_project:204 - Saving project project_a69cd594
2025-10-19 18:07:46 - requirements_management - [32mINFO[0m - call_save_tool:113 - Saving to Neo4j
2025-10-19 18:07:46 - requirements_management - [32mINFO[0m - save_to_neo4j:26 - Saving to Neo4j: project_a69cd594
2025-10-19 18:07:46 - requirements_management - [32mINFO[0m - save_to_neo4j:63 - Saved 5 requirements, 5 risks
2025-10-19 18:07:46 - requirements_management - [32mINFO[0m - call_save_tool:124 - Saved successfully
2025-10-19 18:15:58 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:15:58 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:15:58 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:16:13 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_72d82530
2025-10-19 18:16:13 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 18:16:14 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 18:16:20 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: Scalable manufacturing process
2025-10-19 18:16:20 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:16:20 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: Scalable manufacturing process
2025-10-19 18:16:21 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 18:16:21 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 18:16:21 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 18:16:22 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 18:16:31 - requirements_management - [32mINFO[0m - save_project:204 - Saving project project_72d82530
2025-10-19 18:16:31 - requirements_management - [32mINFO[0m - call_save_tool:113 - Saving to Neo4j
2025-10-19 18:16:31 - requirements_management - [32mINFO[0m - save_to_neo4j:26 - Saving to Neo4j: project_72d82530
2025-10-19 18:16:31 - requirements_management - [32mINFO[0m - save_to_neo4j:63 - Saved 5 requirements, 5 risks
2025-10-19 18:16:31 - requirements_management - [32mINFO[0m - call_save_tool:124 - Saved successfully
2025-10-19 18:23:50 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:23:50 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:23:50 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:25:17 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:25:17 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:25:17 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:26:05 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:26:05 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:26:05 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:27:30 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:27:30 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:27:30 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:30:34 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:30:34 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:30:34 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:30:44 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:30:44 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:30:44 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:35:01 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:35:01 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:35:01 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:39:45 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:39:45 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:39:45 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:41:32 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:41:32 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:41:32 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:41:41 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:41:41 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:41:41 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:41:59 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:41:59 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:41:59 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:42:41 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:42:41 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:42:41 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:42:49 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:42:49 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:42:49 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:51:39 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:51:39 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:51:39 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:52:51 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:52:51 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:52:51 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:53:03 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_e1bcdec9
2025-10-19 18:53:03 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 18:53:04 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 18:53:09 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing processes
2025-10-19 18:53:09 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:53:09 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing processes
2025-10-19 18:53:17 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 18:53:17 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 18:53:17 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 18:53:38 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:53:38 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:53:38 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:53:42 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_1639c30c
2025-10-19 18:53:42 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 18:53:43 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 18:53:47 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing processes
2025-10-19 18:53:47 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:53:47 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing processes
2025-10-19 18:53:49 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 18:53:49 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 18:53:49 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 18:54:04 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 18:54:04 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 18:54:04 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 18:54:08 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_312bc4c4
2025-10-19 18:54:08 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 18:54:09 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 18:54:13 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing process
2025-10-19 18:54:13 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 18:54:13 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing process
2025-10-19 18:54:13 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 18:54:13 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 18:54:13 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 18:54:15 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 18:54:31 - requirements_management - [32mINFO[0m - regenerate:162 - Regenerating risks
2025-10-19 18:54:31 - requirements_management - [32mINFO[0m - regenerate:172 - Regenerating risks
2025-10-19 18:54:31 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 18:54:33 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 19:36:10 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: fuel efficient engine
2025-10-19 19:36:10 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:36:10 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: fuel efficient engine
2025-10-19 19:36:55 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_72d82530
2025-10-19 19:36:55 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:36:56 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing process",\n  "robust after-sales service"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing process",\n  "robust after-sales service"\n]\n```'}}
2025-10-19 19:36:56 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing process",\n  "robust after-sales service"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "low emission compliance",\n  "ergonomic interior design",\n  "scalable manufacturing process",\n  "robust after-sales service"\n]\n```'}}
During task with name 'generate_keywords' and id '0796967c-0324-3522-e2d3-fed94a3782da'
2025-10-19 19:37:14 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_cf9060df
2025-10-19 19:37:14 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:37:19 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:37:23 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing processes
2025-10-19 19:37:23 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:37:23 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing processes
2025-10-19 19:37:24 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:37:24 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:37:24 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:38:05 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 19:38:05 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 19:38:05 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 19:38:12 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_a84833d6
2025-10-19 19:38:12 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:38:14 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:38:17 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing process
2025-10-19 19:38:17 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:38:17 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing process
2025-10-19 19:38:19 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:38:19 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:38:19 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:38:37 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_6c184ce8
2025-10-19 19:38:37 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:38:37 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient performance",\n  "low emission compliance",\n  "ergonomic interior comfort",\n  "scalable manufacturing quality",\n  "robust after-sales support"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient performance",\n  "low emission compliance",\n  "ergonomic interior comfort",\n  "scalable manufacturing quality",\n  "robust after-sales support"\n]\n```'}}
2025-10-19 19:38:37 - requirements_management - [31mERROR[0m - create_project:90 - Error creating project: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient performance",\n  "low emission compliance",\n  "ergonomic interior comfort",\n  "scalable manufacturing quality",\n  "robust after-sales support"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 72, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient performance",\n  "low emission compliance",\n  "ergonomic interior comfort",\n  "scalable manufacturing quality",\n  "robust after-sales support"\n]\n```'}}
During task with name 'generate_keywords' and id '35359c2c-0944-3b23-52ea-172726e51005'
2025-10-19 19:38:56 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_0df1e774
2025-10-19 19:38:56 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:38:59 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:39:03 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: ergonomic interior design
2025-10-19 19:39:03 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:39:03 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior design
2025-10-19 19:39:04 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:39:04 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:39:04 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:39:29 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 19:39:29 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 19:39:29 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 19:39:38 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 19:39:38 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 19:39:38 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 19:39:51 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 19:39:51 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 19:39:51 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 19:40:06 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_667df3e9
2025-10-19 19:40:06 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:40:07 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:40:11 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: scalable manufacturing quality
2025-10-19 19:40:11 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:40:11 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing quality
2025-10-19 19:41:18 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 19:41:18 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 19:41:18 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 19:41:36 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 19:41:36 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 19:41:36 - requirements_management - [32mINFO[0m - startup_event:36 - Starting Requirements Management API v1.0.0
2025-10-19 19:41:36 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_369d4a29
2025-10-19 19:41:36 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:41:37 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:41:43 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: ergonomic interior comfort
2025-10-19 19:41:43 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:41:43 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior comfort
2025-10-19 19:41:45 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:41:45 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:41:45 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:41:48 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 19:42:03 - requirements_management - [32mINFO[0m - regenerate:162 - Regenerating requirements
2025-10-19 19:42:03 - requirements_management - [32mINFO[0m - regenerate:168 - Regenerating requirements
2025-10-19 19:42:03 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior comfort
2025-10-19 19:42:04 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:42:41 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_0e7f92d3
2025-10-19 19:42:41 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:42:41 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:42:46 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: Fuel Efficient Engines
2025-10-19 19:42:46 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:42:46 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: Fuel Efficient Engines
2025-10-19 19:42:47 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:42:47 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:42:47 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:42:48 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 19:42:52 - requirements_management - [32mINFO[0m - save_project:204 - Saving project project_0e7f92d3
2025-10-19 19:42:52 - requirements_management - [32mINFO[0m - call_save_tool:113 - Saving to Neo4j
2025-10-19 19:42:52 - requirements_management - [32mINFO[0m - save_to_neo4j:26 - Saving to Neo4j: project_0e7f92d3
2025-10-19 19:42:53 - requirements_management - [32mINFO[0m - save_to_neo4j:63 - Saved 5 requirements, 5 risks
2025-10-19 19:42:53 - requirements_management - [32mINFO[0m - call_save_tool:124 - Saved successfully
2025-10-19 19:48:14 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project_55161932
2025-10-19 19:48:14 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:48:15 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:48:22 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: Fuel efficient engines
2025-10-19 19:48:22 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:48:22 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: Fuel efficient engines
2025-10-19 19:48:23 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:48:23 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:48:23 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:48:24 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 19:48:56 - requirements_management - [32mINFO[0m - save_project:204 - Saving project project_55161932
2025-10-19 19:48:56 - requirements_management - [32mINFO[0m - call_save_tool:113 - Saving to Neo4j
2025-10-19 19:48:56 - requirements_management - [32mINFO[0m - save_to_neo4j:26 - Saving to Neo4j: project_55161932
2025-10-19 19:48:56 - requirements_management - [32mINFO[0m - save_to_neo4j:63 - Saved 5 requirements, 5 risks
2025-10-19 19:48:56 - requirements_management - [32mINFO[0m - call_save_tool:124 - Saved successfully
2025-10-19 19:56:35 - requirements_management - [32mINFO[0m - create_project:55 - Creating project: project: "project_0e7f92d3"
2025-10-19 19:56:35 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 19:56:35 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 19:56:44 - requirements_management - [32mINFO[0m - select_keyword:115 - Selected keyword: low emission compliance
2025-10-19 19:56:44 - requirements_management - [32mINFO[0m - select_keyword:124 - Invoking generate_requirements
2025-10-19 19:56:44 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: low emission compliance
2025-10-19 19:56:45 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 19:56:45 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_risks
2025-10-19 19:56:45 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 19:56:46 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:00:36 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:00:36 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:00:36 - requirements_management - [32mINFO[0m - startup_event:40 - Starting Requirements Management API v1.0.0
2025-10-19 20:01:28 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:01:28 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:01:28 - requirements_management - [32mINFO[0m - startup_event:40 - Starting Requirements Management API v1.0.0
2025-10-19 20:01:37 - requirements_management - [32mINFO[0m - create_project:59 - Creating project: 0e7f92d3
2025-10-19 20:01:37 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:01:38 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:01:40 - requirements_management - [32mINFO[0m - select_keyword:119 - Selected keyword: fuel efficient engines
2025-10-19 20:01:40 - requirements_management - [32mINFO[0m - select_keyword:128 - Invoking generate_requirements
2025-10-19 20:01:40 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: fuel efficient engines
2025-10-19 20:01:46 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:01:46 - requirements_management - [32mINFO[0m - select_keyword:132 - Invoking generate_risks
2025-10-19 20:01:46 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:01:48 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:02:15 - requirements_management - [32mINFO[0m - regenerate_requirements:213 - Regenerating requirements at indexes: [0]
2025-10-19 20:02:15 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: fuel efficient engines
2025-10-19 20:02:16 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:02:25 - requirements_management - [32mINFO[0m - regenerate_requirements:213 - Regenerating requirements at indexes: [0]
2025-10-19 20:02:25 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: fuel efficient engines
2025-10-19 20:02:26 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:03:03 - requirements_management - [32mINFO[0m - regenerate_requirements:213 - Regenerating requirements at indexes: [0]
2025-10-19 20:03:03 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: fuel efficient engines
2025-10-19 20:03:04 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:03:52 - requirements_management - [32mINFO[0m - regenerate_requirements:213 - Regenerating requirements at indexes: [0]
2025-10-19 20:03:52 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: fuel efficient engines
2025-10-19 20:03:53 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:10:24 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:10:24 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:10:24 - requirements_management - [32mINFO[0m - startup_event:40 - Starting Requirements Management API v1.0.0
2025-10-19 20:10:37 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:10:37 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:10:37 - requirements_management - [32mINFO[0m - startup_event:40 - Starting Requirements Management API v1.0.0
2025-10-19 20:14:25 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:14:25 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:15:50 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:15:50 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:16:35 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:16:35 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:17:19 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:17:19 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:18:02 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:18:02 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:18:02 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:18:10 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:18:10 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:18:10 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:20:16 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:20:16 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:20:16 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:24:06 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:24:06 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:24:06 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:25:30 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: test
2025-10-19 20:25:31 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:25:32 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:25:59 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_da1bee59
2025-10-19 20:25:59 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:26:00 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:26:03 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior comfort
2025-10-19 20:26:03 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:26:03 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior comfort
2025-10-19 20:26:04 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:26:04 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:26:04 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:26:05 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:26:46 - requirements_management - [32mINFO[0m - regenerate_with_feedback:544 - Regenerating requirement with feedback: update fuel efficiency to 90
2025-10-19 20:27:23 - requirements_management - [32mINFO[0m - regenerate_with_feedback:544 - Regenerating requirement with feedback: update fuel efficiency to 50mpg
2025-10-19 20:28:46 - requirements_management - [32mINFO[0m - update_single_risk:431 - Updating risk at index 0 in Neo4j
2025-10-19 20:33:07 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:33:07 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:33:07 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:33:08 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_c6bae511
2025-10-19 20:33:08 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:33:15 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:33:15 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_0c84fce8
2025-10-19 20:33:15 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:33:15 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:33:20 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: robust after-sales service
2025-10-19 20:33:20 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:33:20 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: robust after-sales service
2025-10-19 20:33:24 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:33:24 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:33:24 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:33:26 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:36:09 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:36:09 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:36:09 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:36:11 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_68e20e10
2025-10-19 20:36:11 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:36:12 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:36:16 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior design
2025-10-19 20:36:16 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:36:16 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior design
2025-10-19 20:36:17 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:36:17 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:36:17 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:36:28 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_3dbbfd4d
2025-10-19 20:36:28 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:36:29 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["fuel efficient engine","low emission compliance","ergonomic interior design","scalable manufacturing process","robust aftersales service"]'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["fuel efficient engine","low emission compliance","ergonomic interior design","scalable manufacturing process","robust aftersales service"]'}}
2025-10-19 20:36:29 - requirements_management - [31mERROR[0m - create_project:97 - Error creating project: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["fuel efficient engine","low emission compliance","ergonomic interior design","scalable manufacturing process","robust aftersales service"]'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 79, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["fuel efficient engine","low emission compliance","ergonomic interior design","scalable manufacturing process","robust aftersales service"]'}}
During task with name 'generate_keywords' and id '48348693-91d6-5328-0bd6-81f3da796756'
2025-10-19 20:36:45 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:36:45 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:36:45 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:36:48 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_cb51d345
2025-10-19 20:36:49 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:36:49 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:36:54 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: low emission compliance
2025-10-19 20:36:54 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:36:54 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: low emission compliance
2025-10-19 20:36:55 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:36:55 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:36:55 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:38:03 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:38:03 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:38:03 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:38:06 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_154085d1
2025-10-19 20:38:06 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:38:07 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["Fuel Efficient Engines","Low Emission Compliance","Ergonomic Interior Comfort","Scalable Manufacturing Process","Robust After Sales"]'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["Fuel Efficient Engines","Low Emission Compliance","Ergonomic Interior Comfort","Scalable Manufacturing Process","Robust After Sales"]'}}
2025-10-19 20:38:07 - requirements_management - [31mERROR[0m - create_project:97 - Error creating project: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["Fuel Efficient Engines","Low Emission Compliance","Ergonomic Interior Comfort","Scalable Manufacturing Process","Robust After Sales"]'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 79, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["Fuel Efficient Engines","Low Emission Compliance","Ergonomic Interior Comfort","Scalable Manufacturing Process","Robust After Sales"]'}}
During task with name 'generate_keywords' and id '96504150-ac92-a784-ffbd-3ca7a46b79fe'
2025-10-19 20:38:53 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:38:53 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:38:53 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:39:45 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:39:45 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:39:45 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:39:45 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_6d9aa2ff
2025-10-19 20:39:45 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:39:46 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:39:47 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior design
2025-10-19 20:39:47 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:39:47 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior design
2025-10-19 20:39:48 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:39:48 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:39:48 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:39:55 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:40:16 - requirements_management - [32mINFO[0m - regenerate_with_feedback:545 - Regenerating requirement with feedback: add more detail
2025-10-19 20:41:07 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:41:07 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:41:07 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:41:08 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_2a03497f
2025-10-19 20:41:08 - requirements_management - [32mINFO[0m - generate_keywords:27 - Generating keywords
2025-10-19 20:41:09 - requirements_management - [33mWARNING[0m - generate_keywords:48 - Structured output failed, falling back to text parsing: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '["fuel efficient engines", "low emission compliance", "ergonomic interior comfort", "scalable manufacturing processes", "robust after-sales service"]'}}
2025-10-19 20:41:10 - requirements_management - [32mINFO[0m - generate_keywords:73 - Generated 5 keywords
2025-10-19 20:41:13 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: low emission compliance
2025-10-19 20:41:13 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:41:13 - requirements_management - [32mINFO[0m - generate_requirements:91 - Generating requirements for: low emission compliance
2025-10-19 20:41:16 - requirements_management - [32mINFO[0m - generate_requirements:143 - Generated 5 requirements
2025-10-19 20:41:16 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:41:16 - requirements_management - [32mINFO[0m - generate_risks:164 - Generating risks
2025-10-19 20:41:18 - requirements_management - [32mINFO[0m - generate_risks:216 - Generated 5 risks
2025-10-19 20:42:02 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:42:02 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:42:02 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:42:18 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_40733b11
2025-10-19 20:42:19 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:42:29 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:42:31 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior comfort
2025-10-19 20:42:31 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:42:31 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior comfort
2025-10-19 20:42:33 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:42:33 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:42:33 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:42:55 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_21bddb38
2025-10-19 20:42:55 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:42:56 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:43:07 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: after sales support
2025-10-19 20:43:07 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:43:07 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: after sales support
2025-10-19 20:43:08 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:43:08 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:43:08 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:43:11 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:45:00 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:45:00 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:45:00 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:46:18 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:46:19 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:46:19 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:46:37 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:46:37 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:46:37 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:46:43 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_9858b861
2025-10-19 20:46:43 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:46:44 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after sales",\n  "modern infotainment connectivity"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after sales",\n  "modern infotainment connectivity"\n]\n```'}}
2025-10-19 20:46:44 - requirements_management - [31mERROR[0m - create_project:97 - Error creating project: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after sales",\n  "modern infotainment connectivity"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 79, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n  "fuel efficient engines",\n  "ergonomic interior design",\n  "scalable manufacturing processes",\n  "robust after sales",\n  "modern infotainment connectivity"\n]\n```'}}
During task with name 'generate_keywords' and id '8525cfe7-2159-2d80-3dd2-ed4a7e5f074d'
2025-10-19 20:47:12 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:47:12 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:47:12 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:47:14 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_704efeb1
2025-10-19 20:47:14 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:47:15 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:47:17 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior design
2025-10-19 20:47:17 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:47:17 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior design
2025-10-19 20:47:20 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:47:20 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:47:20 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:47:21 - requirements_management - [31mERROR[0m - generate_risks:108 - Error generating risks: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to meet the 5.5\u202fL/100\u202fkm fuel consumption and <120\u202fg/km CO₂ targets could result in non‑compliance with Euro\u202f7 regulations, leading to fines, market restrictions, and damage to brand reputation.",\n    "Inadequate ergonomic design or limited adjustability of seats, steering wheel, or control placement may increase driver fatigue, cause musculoskeletal injuries, and result in higher warranty claims or reduced customer satisfaction.",\n    "Integration issues with the 12‑inch touchscreen, Apple CarPlay/Android Auto, OTA updates, or intelligent climate control could cause software bugs, security vulnerabilities, or system downtime, undermining user experience and safety.",\n    "Scaling production to 200,000 units while keeping costs below $22,000 per vehicle may strain supply chain capacity, leading to component shortages, lower supplier reliability (<95\u202f% score), and quality defects that increase rework costs.",\n    "Inability to deliver the promised 5‑year/100,000‑km warranty, remote diagnostics, or 24/7 multilingual support could result in higher warranty expenses, customer churn, and negative brand perception."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to meet the 5.5\u202fL/100\u202fkm fuel consumption and <120\u202fg/km CO₂ targets could result in non‑compliance with Euro\u202f7 regulations, leading to fines, market restrictions, and damage to brand reputation.",\n    "Inadequate ergonomic design or limited adjustability of seats, steering wheel, or control placement may increase driver fatigue, cause musculoskeletal injuries, and result in higher warranty claims or reduced customer satisfaction.",\n    "Integration issues with the 12‑inch touchscreen, Apple CarPlay/Android Auto, OTA updates, or intelligent climate control could cause software bugs, security vulnerabilities, or system downtime, undermining user experience and safety.",\n    "Scaling production to 200,000 units while keeping costs below $22,000 per vehicle may strain supply chain capacity, leading to component shortages, lower supplier reliability (<95\u202f% score), and quality defects that increase rework costs.",\n    "Inability to deliver the promised 5‑year/100,000‑km warranty, remote diagnostics, or 24/7 multilingual support could result in higher warranty expenses, customer churn, and negative brand perception."\n  ]\n}}'}}
2025-10-19 20:47:21 - requirements_management - [31mERROR[0m - select_keyword:152 - Error selecting keyword: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to meet the 5.5\u202fL/100\u202fkm fuel consumption and <120\u202fg/km CO₂ targets could result in non‑compliance with Euro\u202f7 regulations, leading to fines, market restrictions, and damage to brand reputation.",\n    "Inadequate ergonomic design or limited adjustability of seats, steering wheel, or control placement may increase driver fatigue, cause musculoskeletal injuries, and result in higher warranty claims or reduced customer satisfaction.",\n    "Integration issues with the 12‑inch touchscreen, Apple CarPlay/Android Auto, OTA updates, or intelligent climate control could cause software bugs, security vulnerabilities, or system downtime, undermining user experience and safety.",\n    "Scaling production to 200,000 units while keeping costs below $22,000 per vehicle may strain supply chain capacity, leading to component shortages, lower supplier reliability (<95\u202f% score), and quality defects that increase rework costs.",\n    "Inability to deliver the promised 5‑year/100,000‑km warranty, remote diagnostics, or 24/7 multilingual support could result in higher warranty expenses, customer churn, and negative brand perception."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 136, in select_keyword
    state = generate_risks(state)
            ^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to meet the 5.5\u202fL/100\u202fkm fuel consumption and <120\u202fg/km CO₂ targets could result in non‑compliance with Euro\u202f7 regulations, leading to fines, market restrictions, and damage to brand reputation.",\n    "Inadequate ergonomic design or limited adjustability of seats, steering wheel, or control placement may increase driver fatigue, cause musculoskeletal injuries, and result in higher warranty claims or reduced customer satisfaction.",\n    "Integration issues with the 12‑inch touchscreen, Apple CarPlay/Android Auto, OTA updates, or intelligent climate control could cause software bugs, security vulnerabilities, or system downtime, undermining user experience and safety.",\n    "Scaling production to 200,000 units while keeping costs below $22,000 per vehicle may strain supply chain capacity, leading to component shortages, lower supplier reliability (<95\u202f% score), and quality defects that increase rework costs.",\n    "Inability to deliver the promised 5‑year/100,000‑km warranty, remote diagnostics, or 24/7 multilingual support could result in higher warranty expenses, customer churn, and negative brand perception."\n  ]\n}}'}}
2025-10-19 20:47:39 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: low emission compliance
2025-10-19 20:47:39 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:47:39 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: low emission compliance
2025-10-19 20:47:40 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:47:40 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:47:40 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:47:41 - requirements_management - [31mERROR[0m - generate_risks:108 - Error generating risks: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to consistently achieve the 95\u202fg/km CO₂ limit across all real‑world operating conditions, potentially leading to non‑compliance with Euro\u202f7 regulations and costly re‑certification efforts.",\n    "Inability to simultaneously meet the 6.0\u202fL/100\u202fkm fuel‑economy target and the 150\u202fkW peak power requirement, risking either reduced vehicle performance or higher emissions than specified.",\n    "Integration of the 10‑inch infotainment system and low‑power electronics may exceed power‑budget constraints or compromise ergonomic standards, resulting in driver distraction, reduced usability, or increased vehicle energy consumption.",\n    "Scaling production to 200,000 units while limiting per‑unit cost increase to ≤5\u202f% could strain statistical process control and the ISO/TS\u202f16949 quality framework, leading to higher defect rates, supply‑chain disruptions, or cost overruns.",\n    "The 5‑year/100,000\u202fkm warranty combined with OTA remote diagnostics may expose the vehicle to cybersecurity vulnerabilities and create warranty cost overruns if critical support responses exceed the ≤24‑hour target."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to consistently achieve the 95\u202fg/km CO₂ limit across all real‑world operating conditions, potentially leading to non‑compliance with Euro\u202f7 regulations and costly re‑certification efforts.",\n    "Inability to simultaneously meet the 6.0\u202fL/100\u202fkm fuel‑economy target and the 150\u202fkW peak power requirement, risking either reduced vehicle performance or higher emissions than specified.",\n    "Integration of the 10‑inch infotainment system and low‑power electronics may exceed power‑budget constraints or compromise ergonomic standards, resulting in driver distraction, reduced usability, or increased vehicle energy consumption.",\n    "Scaling production to 200,000 units while limiting per‑unit cost increase to ≤5\u202f% could strain statistical process control and the ISO/TS\u202f16949 quality framework, leading to higher defect rates, supply‑chain disruptions, or cost overruns.",\n    "The 5‑year/100,000\u202fkm warranty combined with OTA remote diagnostics may expose the vehicle to cybersecurity vulnerabilities and create warranty cost overruns if critical support responses exceed the ≤24‑hour target."\n  ]\n}}'}}
2025-10-19 20:47:41 - requirements_management - [31mERROR[0m - select_keyword:152 - Error selecting keyword: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to consistently achieve the 95\u202fg/km CO₂ limit across all real‑world operating conditions, potentially leading to non‑compliance with Euro\u202f7 regulations and costly re‑certification efforts.",\n    "Inability to simultaneously meet the 6.0\u202fL/100\u202fkm fuel‑economy target and the 150\u202fkW peak power requirement, risking either reduced vehicle performance or higher emissions than specified.",\n    "Integration of the 10‑inch infotainment system and low‑power electronics may exceed power‑budget constraints or compromise ergonomic standards, resulting in driver distraction, reduced usability, or increased vehicle energy consumption.",\n    "Scaling production to 200,000 units while limiting per‑unit cost increase to ≤5\u202f% could strain statistical process control and the ISO/TS\u202f16949 quality framework, leading to higher defect rates, supply‑chain disruptions, or cost overruns.",\n    "The 5‑year/100,000\u202fkm warranty combined with OTA remote diagnostics may expose the vehicle to cybersecurity vulnerabilities and create warranty cost overruns if critical support responses exceed the ≤24‑hour target."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 136, in select_keyword
    state = generate_risks(state)
            ^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Failure to consistently achieve the 95\u202fg/km CO₂ limit across all real‑world operating conditions, potentially leading to non‑compliance with Euro\u202f7 regulations and costly re‑certification efforts.",\n    "Inability to simultaneously meet the 6.0\u202fL/100\u202fkm fuel‑economy target and the 150\u202fkW peak power requirement, risking either reduced vehicle performance or higher emissions than specified.",\n    "Integration of the 10‑inch infotainment system and low‑power electronics may exceed power‑budget constraints or compromise ergonomic standards, resulting in driver distraction, reduced usability, or increased vehicle energy consumption.",\n    "Scaling production to 200,000 units while limiting per‑unit cost increase to ≤5\u202f% could strain statistical process control and the ISO/TS\u202f16949 quality framework, leading to higher defect rates, supply‑chain disruptions, or cost overruns.",\n    "The 5‑year/100,000\u202fkm warranty combined with OTA remote diagnostics may expose the vehicle to cybersecurity vulnerabilities and create warranty cost overruns if critical support responses exceed the ≤24‑hour target."\n  ]\n}}'}}
2025-10-19 20:47:47 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_51ff2640
2025-10-19 20:47:47 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:47:48 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:47:49 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: scalable manufacturing process
2025-10-19 20:47:49 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:47:49 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: scalable manufacturing process
2025-10-19 20:47:50 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:47:50 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:47:50 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:47:51 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:48:07 - requirements_management - [32mINFO[0m - regenerate_with_feedback:564 - Regenerating requirement with feedback: 9002 standard
2025-10-19 20:51:29 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_5a059bc4
2025-10-19 20:51:29 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:51:29 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:51:32 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: comfortable ergonomic interior
2025-10-19 20:51:32 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:51:32 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: comfortable ergonomic interior
2025-10-19 20:51:33 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:51:33 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:51:33 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:51:34 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:51:54 - requirements_management - [32mINFO[0m - regenerate_with_feedback:564 - Regenerating requirement with feedback: fix the issues
2025-10-19 20:52:20 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_c54cfd8f
2025-10-19 20:52:20 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:52:21 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:52:22 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior design
2025-10-19 20:52:22 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:52:22 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior design
2025-10-19 20:52:23 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:52:23 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:52:23 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:52:24 - requirements_management - [31mERROR[0m - generate_risks:108 - Error generating risks: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic target may be unattainable due to limited seat adjustability or insufficient human‑factor testing, leading to lower comfort scores and potential regulatory non‑compliance.",\n    "Powertrain performance targets could conflict with emission limits, risking failure to meet Euro\u202f6d standards or requiring costly engine redesigns to achieve the 5.5\u202fL/100\u202fkm fuel consumption and 0‑60\u202fmph ≤9.5\u202fs goal.",\n    "Integrating all modules on a unified CAN architecture while meeting ISO\u202f26262 safety may expose the vehicle to software integration bugs or unsafe communication failures, especially during OTA updates.",\n    "Lean manufacturing and cost‑reduction targets may pressure supplier quality and delivery performance, increasing the risk of component shortages, rework, or compromised product quality.",\n    "The ambitious warranty and digital service promises could be undermined by insufficient service network capacity or inaccurate scheduling algorithms, leading to missed appointment reductions and lower customer satisfaction scores."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic target may be unattainable due to limited seat adjustability or insufficient human‑factor testing, leading to lower comfort scores and potential regulatory non‑compliance.",\n    "Powertrain performance targets could conflict with emission limits, risking failure to meet Euro\u202f6d standards or requiring costly engine redesigns to achieve the 5.5\u202fL/100\u202fkm fuel consumption and 0‑60\u202fmph ≤9.5\u202fs goal.",\n    "Integrating all modules on a unified CAN architecture while meeting ISO\u202f26262 safety may expose the vehicle to software integration bugs or unsafe communication failures, especially during OTA updates.",\n    "Lean manufacturing and cost‑reduction targets may pressure supplier quality and delivery performance, increasing the risk of component shortages, rework, or compromised product quality.",\n    "The ambitious warranty and digital service promises could be undermined by insufficient service network capacity or inaccurate scheduling algorithms, leading to missed appointment reductions and lower customer satisfaction scores."\n  ]\n}}'}}
2025-10-19 20:52:24 - requirements_management - [31mERROR[0m - select_keyword:152 - Error selecting keyword: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic target may be unattainable due to limited seat adjustability or insufficient human‑factor testing, leading to lower comfort scores and potential regulatory non‑compliance.",\n    "Powertrain performance targets could conflict with emission limits, risking failure to meet Euro\u202f6d standards or requiring costly engine redesigns to achieve the 5.5\u202fL/100\u202fkm fuel consumption and 0‑60\u202fmph ≤9.5\u202fs goal.",\n    "Integrating all modules on a unified CAN architecture while meeting ISO\u202f26262 safety may expose the vehicle to software integration bugs or unsafe communication failures, especially during OTA updates.",\n    "Lean manufacturing and cost‑reduction targets may pressure supplier quality and delivery performance, increasing the risk of component shortages, rework, or compromised product quality.",\n    "The ambitious warranty and digital service promises could be undermined by insufficient service network capacity or inaccurate scheduling algorithms, leading to missed appointment reductions and lower customer satisfaction scores."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 136, in select_keyword
    state = generate_risks(state)
            ^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic target may be unattainable due to limited seat adjustability or insufficient human‑factor testing, leading to lower comfort scores and potential regulatory non‑compliance.",\n    "Powertrain performance targets could conflict with emission limits, risking failure to meet Euro\u202f6d standards or requiring costly engine redesigns to achieve the 5.5\u202fL/100\u202fkm fuel consumption and 0‑60\u202fmph ≤9.5\u202fs goal.",\n    "Integrating all modules on a unified CAN architecture while meeting ISO\u202f26262 safety may expose the vehicle to software integration bugs or unsafe communication failures, especially during OTA updates.",\n    "Lean manufacturing and cost‑reduction targets may pressure supplier quality and delivery performance, increasing the risk of component shortages, rework, or compromised product quality.",\n    "The ambitious warranty and digital service promises could be undermined by insufficient service network capacity or inaccurate scheduling algorithms, leading to missed appointment reductions and lower customer satisfaction scores."\n  ]\n}}'}}
2025-10-19 20:52:28 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_e2b5888f
2025-10-19 20:52:28 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:52:29 - requirements_management - [31mERROR[0m - generate_keywords:46 - Error generating keywords: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n    "Fuel Efficient Engines",\n    "Low Emission Compliance",\n    "Ergonomic Interior Design",\n    "Scalable Manufacturing Process",\n    "Robust After Sales"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n    "Fuel Efficient Engines",\n    "Low Emission Compliance",\n    "Ergonomic Interior Design",\n    "Scalable Manufacturing Process",\n    "Robust After Sales"\n]\n```'}}
2025-10-19 20:52:29 - requirements_management - [31mERROR[0m - create_project:97 - Error creating project: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n    "Fuel Efficient Engines",\n    "Low Emission Compliance",\n    "Ergonomic Interior Design",\n    "Scalable Manufacturing Process",\n    "Robust After Sales"\n]\n```'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 79, in create_project
    for event in workflow_graph.stream(state, thread, stream_mode="values"):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\main.py", line 2674, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_runner.py", line 162, in tick
    run_with_retry(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\pregel\_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 657, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langgraph\_internal\_runnable.py", line 401, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 37, in generate_keywords
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'Tool choice is required, but model did not call a tool', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '```json\n[\n    "Fuel Efficient Engines",\n    "Low Emission Compliance",\n    "Ergonomic Interior Design",\n    "Scalable Manufacturing Process",\n    "Robust After Sales"\n]\n```'}}
During task with name 'generate_keywords' and id '6724fc32-05cd-4ad6-c61f-eca824eb554b'
2025-10-19 20:52:44 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_5bb1e565
2025-10-19 20:52:44 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:52:44 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:52:47 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior comfort
2025-10-19 20:52:47 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:52:47 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior comfort
2025-10-19 20:52:47 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:52:47 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:52:47 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:52:48 - requirements_management - [31mERROR[0m - generate_risks:108 - Error generating risks: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic rating target may not be met due to variability in anthropometric data and seat design constraints, leading to lower driver/passenger comfort scores.",\n    "Climate control system might fail to maintain the ±2°C temperature variance within 30 minutes due to sensor drift, extreme ambient conditions, or insufficient zone isolation, causing passenger discomfort.",\n    "Unified CAN‑based infotainment architecture could experience bandwidth saturation or software integration bugs, resulting in response times exceeding the 200\u202fms limit and degraded user experience.",\n    "Scaling manufacturing to 150,000 units may strain SPC and automated defect detection capabilities, increasing the risk of batch defect rates surpassing the 0.5\u202f% threshold.",\n    "Warranty management platform may encounter data latency or workflow bottlenecks, preventing real‑time claim tracking and the 7‑business‑day resolution guarantee, which could damage customer trust."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic rating target may not be met due to variability in anthropometric data and seat design constraints, leading to lower driver/passenger comfort scores.",\n    "Climate control system might fail to maintain the ±2°C temperature variance within 30 minutes due to sensor drift, extreme ambient conditions, or insufficient zone isolation, causing passenger discomfort.",\n    "Unified CAN‑based infotainment architecture could experience bandwidth saturation or software integration bugs, resulting in response times exceeding the 200\u202fms limit and degraded user experience.",\n    "Scaling manufacturing to 150,000 units may strain SPC and automated defect detection capabilities, increasing the risk of batch defect rates surpassing the 0.5\u202f% threshold.",\n    "Warranty management platform may encounter data latency or workflow bottlenecks, preventing real‑time claim tracking and the 7‑business‑day resolution guarantee, which could damage customer trust."\n  ]\n}}'}}
2025-10-19 20:52:49 - requirements_management - [31mERROR[0m - select_keyword:152 - Error selecting keyword: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic rating target may not be met due to variability in anthropometric data and seat design constraints, leading to lower driver/passenger comfort scores.",\n    "Climate control system might fail to maintain the ±2°C temperature variance within 30 minutes due to sensor drift, extreme ambient conditions, or insufficient zone isolation, causing passenger discomfort.",\n    "Unified CAN‑based infotainment architecture could experience bandwidth saturation or software integration bugs, resulting in response times exceeding the 200\u202fms limit and degraded user experience.",\n    "Scaling manufacturing to 150,000 units may strain SPC and automated defect detection capabilities, increasing the risk of batch defect rates surpassing the 0.5\u202f% threshold.",\n    "Warranty management platform may encounter data latency or workflow bottlenecks, preventing real‑time claim tracking and the 7‑business‑day resolution guarantee, which could damage customer trust."\n  ]\n}}'}}
Traceback (most recent call last):
  File "H:\akash\git\CoherenceAI\Req-Management\backend\api.py", line 136, in select_keyword
    state = generate_risks(state)
            ^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\backend\nodes.py", line 95, in generate_risks
    result = (prompt | llm_structured).invoke({
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3246, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5711, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1025, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 842, in generate
    self._generate_with_cache(
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1091, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 533, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 456, in create
    return self._post(
           ^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\akash\git\CoherenceAI\Req-Management\.venv\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': "Tool call validation failed: tool call validation failed: attempted to call tool 'json' which was not in request.tools", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '{"name": "json", "arguments": {\n  "Risks": [\n    "Ergonomic rating target may not be met due to variability in anthropometric data and seat design constraints, leading to lower driver/passenger comfort scores.",\n    "Climate control system might fail to maintain the ±2°C temperature variance within 30 minutes due to sensor drift, extreme ambient conditions, or insufficient zone isolation, causing passenger discomfort.",\n    "Unified CAN‑based infotainment architecture could experience bandwidth saturation or software integration bugs, resulting in response times exceeding the 200\u202fms limit and degraded user experience.",\n    "Scaling manufacturing to 150,000 units may strain SPC and automated defect detection capabilities, increasing the risk of batch defect rates surpassing the 0.5\u202f% threshold.",\n    "Warranty management platform may encounter data latency or workflow bottlenecks, preventing real‑time claim tracking and the 7‑business‑day resolution guarantee, which could damage customer trust."\n  ]\n}}'}}
2025-10-19 20:53:04 - requirements_management - [32mINFO[0m - build_graph:32 - Building workflow graph
2025-10-19 20:53:04 - requirements_management - [32mINFO[0m - build_graph:66 - Graph built successfully
2025-10-19 20:53:04 - requirements_management - [32mINFO[0m - startup_event:43 - Starting Requirements Management API v1.0.0
2025-10-19 20:53:08 - requirements_management - [32mINFO[0m - create_project:62 - Creating project: project_1abf502f
2025-10-19 20:53:08 - requirements_management - [32mINFO[0m - generate_keywords:26 - Generating keywords
2025-10-19 20:53:09 - requirements_management - [32mINFO[0m - generate_keywords:43 - Generated 5 keywords
2025-10-19 20:53:16 - requirements_management - [32mINFO[0m - select_keyword:122 - Selected keyword: ergonomic interior comfort
2025-10-19 20:53:16 - requirements_management - [32mINFO[0m - select_keyword:131 - Invoking generate_requirements
2025-10-19 20:53:16 - requirements_management - [32mINFO[0m - generate_requirements:51 - Generating requirements for: ergonomic interior comfort
2025-10-19 20:53:17 - requirements_management - [32mINFO[0m - generate_requirements:68 - Generated 5 requirements
2025-10-19 20:53:17 - requirements_management - [32mINFO[0m - select_keyword:135 - Invoking generate_risks
2025-10-19 20:53:17 - requirements_management - [32mINFO[0m - generate_risks:80 - Generating risks
2025-10-19 20:53:19 - requirements_management - [32mINFO[0m - generate_risks:101 - Generated 5 risks
2025-10-19 20:53:34 - requirements_management - [32mINFO[0m - regenerate_with_feedback:564 - Regenerating requirement with feedback: 90 g/km
2025-10-19 20:56:21 - requirements_management - [32mINFO[0m - regenerate_with_feedback:564 - Regenerating requirement with feedback: change to 10seconds or later
